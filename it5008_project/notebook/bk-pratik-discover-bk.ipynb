{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13d03ea0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from collections import defaultdict\n",
    "from itertools import combinations\n",
    "\n",
    "def _attr_bitmask(attrs, attr_to_bit):\n",
    "    \"\"\"Helper to build a bitmask from an iterable of attribute names.\"\"\"\n",
    "    m = 0\n",
    "    for a in attrs:\n",
    "        m |= 1 << attr_to_bit[a]\n",
    "    return m\n",
    "\n",
    "def _bit_to_attrs(bitmask, bit_to_attr):\n",
    "    \"\"\"Inverse of _attr_bitmask.\"\"\"\n",
    "    res = []\n",
    "    i = 0\n",
    "    while bitmask:\n",
    "        if bitmask & 1:\n",
    "            res.append(bit_to_attr[i])\n",
    "        bitmask >>= 1\n",
    "        i += 1\n",
    "    return res\n",
    "\n",
    "def _partition(df, cols):\n",
    "    \"\"\"\n",
    "    Build a partition (list of frozenset row indices) for a set of columns.\n",
    "    Two rows are in the same block iff they have identical values on 'cols'.\n",
    "    \"\"\"\n",
    "    if not cols:\n",
    "        # Single block containing all row indices\n",
    "        return [frozenset(range(len(df)))]\n",
    "    groups = defaultdict(list)\n",
    "    view = df[list(cols)].itertuples(index=False, name=None)\n",
    "    for i, key in enumerate(view):\n",
    "        groups[key].append(i)\n",
    "    return [frozenset(g) for g in groups.values()]\n",
    "\n",
    "def _partition_cardinality(part):\n",
    "    \"\"\"Number of distinct value-combinations = number of blocks in the partition.\"\"\"\n",
    "    return len(part)\n",
    "\n",
    "def _refine_partition(part_left, part_right, nrows):\n",
    "    \"\"\"\n",
    "    Compute partition of the union of attribute sets if we already know\n",
    "    partitions of the two sets (Armstrong refinement). Equivalent to chasing\n",
    "    equalities: blocks become intersections.\n",
    "    \"\"\"\n",
    "    # Map row -> block id for each partition\n",
    "    left_pos = [None]*nrows\n",
    "    right_pos = [None]*nrows\n",
    "    for bid, block in enumerate(part_left):\n",
    "        for r in block:\n",
    "            left_pos[r] = bid\n",
    "    for bid, block in enumerate(part_right):\n",
    "        for r in block:\n",
    "            right_pos[r] = bid\n",
    "\n",
    "    # Intersection blocks\n",
    "    inter = defaultdict(list)\n",
    "    for r in range(nrows):\n",
    "        inter[(left_pos[r], right_pos[r])].append(r)\n",
    "    return [frozenset(v) for v in inter.values()]\n",
    "\n",
    "def discover_fds_with_chase(df: pd.DataFrame, max_lhs=None):\n",
    "    \"\"\"\n",
    "    Discover a minimal cover of FDs X -> A from a pandas DataFrame using a chase-style\n",
    "    partition refinement. Returns a list of (lhs_tuple, rhs_attr) with lhs sorted.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    df : pd.DataFrame\n",
    "        Input table (duplicates allowed; duplicates don't affect FDs).\n",
    "    max_lhs : int | None\n",
    "        Optional cap on the size of LHS to control runtime on wide tables.\n",
    "\n",
    "    Notes\n",
    "    -----\n",
    "    - X -> A holds iff #blocks(X) == #blocks(X ∪ {A})\n",
    "    - We build partitions level-wise and reuse refinements to avoid recomputation.\n",
    "    - We prune supersets using discovered minimal LHSs.\n",
    "    \"\"\"\n",
    "    cols = list(df.columns)\n",
    "    n = len(cols)\n",
    "    nrows = len(df)\n",
    "    if n == 0:\n",
    "        return []\n",
    "\n",
    "    # Bit encodings for fast subset operations\n",
    "    attr_to_bit = {a:i for i, a in enumerate(cols)}\n",
    "    bit_to_attr = {i:a for a, i in attr_to_bit.items()}\n",
    "\n",
    "    # Cache partitions by bitmask\n",
    "    part_cache: dict[int, list[frozenset[int]]] = {}\n",
    "\n",
    "    # Single-attribute partitions\n",
    "    for a in cols:\n",
    "        b = 1 << attr_to_bit[a]\n",
    "        part_cache[b] = _partition(df, [a])\n",
    "\n",
    "    # Empty set partition\n",
    "    part_cache[0] = _partition(df, [])  # one block of all rows\n",
    "\n",
    "    # Utility to get partition from cache, refining if needed\n",
    "    def get_partition(bitmask: int) -> list[frozenset[int]]:\n",
    "        if bitmask in part_cache:\n",
    "            return part_cache[bitmask]\n",
    "        # Split into two non-empty parts to refine\n",
    "        # Use last set bit as singleton to refine incrementally\n",
    "        b = bitmask & -bitmask               # least significant set bit\n",
    "        rest = bitmask ^ b\n",
    "        p_left = get_partition(rest)\n",
    "        p_right = get_partition(b)\n",
    "        part = _refine_partition(p_left, p_right, nrows)\n",
    "        part_cache[bitmask] = part\n",
    "        return part\n",
    "\n",
    "    # Candidates: for each RHS attribute A, find minimal X ⊆ R\\{A} such that X -> A\n",
    "    # We do a BFS over subset sizes, with pruning by discovered minimal LHSs.\n",
    "    fds = []  # (tuple(lhs_names), rhs_name)\n",
    "    for rhs in cols:\n",
    "        rhs_bit = 1 << attr_to_bit[rhs]\n",
    "        attrs_wo_rhs = [a for a in cols if a != rhs]\n",
    "\n",
    "        # Known minimal LHSs for this rhs (as bitmasks), to prune supersets\n",
    "        minimal_lhss: list[int] = []\n",
    "\n",
    "        # Level-wise exploration\n",
    "        max_k = (max_lhs if max_lhs is not None else len(attrs_wo_rhs))\n",
    "        for k in range(0, max_k + 1):\n",
    "            level_candidates = []\n",
    "            for comb in combinations(attrs_wo_rhs, k):\n",
    "                bm = _attr_bitmask(comb, attr_to_bit)\n",
    "\n",
    "                # Prune if it has a known minimal subset already\n",
    "                skip = False\n",
    "                for m in minimal_lhss:\n",
    "                    if m & bm == m:  # m ⊆ bm\n",
    "                        skip = True\n",
    "                        break\n",
    "                if skip:\n",
    "                    continue\n",
    "                level_candidates.append(bm)\n",
    "\n",
    "            if not level_candidates:\n",
    "                continue\n",
    "\n",
    "            # Test candidates with partition cardinalities (chase of equalities)\n",
    "            for bm in level_candidates:\n",
    "                pX = get_partition(bm)\n",
    "                pXA = get_partition(bm | rhs_bit)\n",
    "                if _partition_cardinality(pX) == _partition_cardinality(pXA):\n",
    "                    # Found X -> rhs; try to minimize X (standard left-reduction)\n",
    "                    # Remove extraneous attributes greedily\n",
    "                    X = bm\n",
    "                    for a in _bit_to_attrs(bm, bit_to_attr):\n",
    "                        abit = 1 << attr_to_bit[a]\n",
    "                        if X & abit:\n",
    "                            X2 = X ^ abit\n",
    "                            pX2 = get_partition(X2)\n",
    "                            pX2A = get_partition(X2 | rhs_bit)\n",
    "                            if _partition_cardinality(pX2) == _partition_cardinality(pX2A):\n",
    "                                X = X2\n",
    "                    minimal_lhss.append(X)\n",
    "                    fds.append((\n",
    "                        tuple(sorted(_bit_to_attrs(X, bit_to_attr))),\n",
    "                        rhs\n",
    "                    ))\n",
    "            # If we already found the empty LHS (i.e., ∅ -> rhs), nothing smaller exists\n",
    "            if any(m == 0 for m in minimal_lhss):\n",
    "                break\n",
    "\n",
    "    # Remove redundant FDs across RHS with transitive minimization:\n",
    "    # Compute a canonical minimal cover (simple pass).\n",
    "    # Build dict rhs -> list of LHS bitmasks, then remove supersets.\n",
    "    per_rhs = defaultdict(list)\n",
    "    for lhs, r in fds:\n",
    "        per_rhs[r].append(_attr_bitmask(lhs, attr_to_bit))\n",
    "    minimal_cover = []\n",
    "    for r, lhs_list in per_rhs.items():\n",
    "        # Remove any LHS that is a superset of another LHS for same RHS\n",
    "        lhs_list = sorted(set(lhs_list), key=lambda x: (bin(x).count(\"1\"), x))\n",
    "        keep = []\n",
    "        for i, x in enumerate(lhs_list):\n",
    "            if any((y & x) == y for j, y in enumerate(lhs_list) if j != i):\n",
    "                # x has a proper subset y in the set; drop x\n",
    "                continue\n",
    "            keep.append(x)\n",
    "        for bm in keep:\n",
    "            minimal_cover.append((tuple(sorted(_bit_to_attrs(bm, bit_to_attr))), r))\n",
    "\n",
    "    # Sort nicely\n",
    "    minimal_cover.sort(key=lambda t: (t[1], len(t[0]), t[0]))\n",
    "    return minimal_cover\n",
    "\n",
    "def group_fds(fds):\n",
    "    \"\"\"\n",
    "    Group functional dependencies by LHS.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    fds : list of (tuple(str), str)\n",
    "        List of FDs where each FD is (lhs_tuple, rhs).\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    dict\n",
    "        Mapping {lhs_tuple: set of rhs attributes}\n",
    "    \"\"\"\n",
    "    grouped = defaultdict(set)\n",
    "    for lhs, rhs in fds:\n",
    "        grouped[tuple(lhs)].add(rhs)\n",
    "    fds = []\n",
    "    for lhs, rhss in grouped.items():\n",
    "        fds.append(tuple([set(lhs), set(rhss)]))\n",
    "    return fds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d21bab03",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = {\n",
    "    \"A\": [1,1,2,2,2,3],\n",
    "    \"B\": [5,5,6,6,7,8],\n",
    "    \"C\": [9,9,9,9,10,11],\n",
    "    \"D\": [0,0,1,1,1,2]\n",
    "}\n",
    "df = pd.DataFrame(data)\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a220af0",
   "metadata": {},
   "outputs": [],
   "source": [
    "fds = discover_fds_with_chase(df)\n",
    "fds = group_fds(fds)\n",
    "for lhs, rhs_set in fds:\n",
    "    # lhs_str = \"{\" + \",\".join(lhs) + \"}\"\n",
    "    # rhs_str = \"{\" + \",\".join(sorted(rhs_set)) + \"}\"\n",
    "    print(f\"{lhs} -> {rhs_set}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07b5724f",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = {\n",
    "    \"StudentID\": [1,1,2,2,3,3,4,4],\n",
    "    \"CourseID\":  [\"C1\",\"C2\",\"C1\",\"C3\",\"C2\",\"C3\",\"C1\",\"C2\"],\n",
    "    \"Dept\":      [\"Math\",\"Math\",\"CS\",\"CS\",\"Math\",\"Math\",\"CS\",\"CS\"],\n",
    "    \"Teacher\":   [\"T1\",\"T1\",\"T2\",\"T2\",\"T1\",\"T1\",\"T2\",\"T2\"],\n",
    "    \"Grade\":     [\"A\",\"B\",\"D\",\"C\",\"B\",\"C\",\"A\",\"C\"]\n",
    "}\n",
    "df = pd.DataFrame(data)\n",
    "print(\"DataFrame:\")\n",
    "print(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c77e03d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "fds = discover_fds_with_chase(df)\n",
    "grouped = group_fds(fds)\n",
    "\n",
    "print(\"\\nDiscovered Functional Dependencies:\")\n",
    "for lhs, rhs_set in grouped:\n",
    "    print(f\"{lhs} -> {rhs_set}\")"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
